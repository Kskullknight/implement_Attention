# Self Attention
## 기존 Attention과 차이점
### 문장 하나에 대하여 어텐션을 진행함
- 이전까지는 <span style="color:rgb(0, 176, 240)">출력 문장의 디코더 히든 스테이트</span>와 <span style="color:rgb(255, 0, 0)">입력 문장의 인코더의 히든 스테이트</span>의 대해서 어텐션을 하는데 Self Attention은 디코더에서 <span style="color:rgb(146, 208, 80)">입력 문장과 입력 문장에 대해서 어텐션</span>을 진행함
=> 자기 문장 내에서 단어들 간의 관계를 찾는다.
=> 그래서 해당 단어를 번역할 때 영향을 미치는 단어를 찾겠다.
- 루옹 어텐션에서 스케일 값이 추가됨

## 의미론적? K, Q, V 이해하기
### 임베딩 이해하기
- 한국과 한글의 차이 => 언어라는 의미적 차이와
- 중국과 중국어의 차이 => 언어라는 의미적 차이
- => 일본에서 위에서 동일한 차이 만큼 이동하면 일본어라는 단어를 의미하는 벡터값이 된다.
> 즉 한 단어 벡터에서 의미적 차이 만큼 벡터를 이동하면 단어에 의미를 포함한 단어의 벡터를 얻을 수 있다.
- 예시
	- ㅁㄴㅇㄹㅁ
#### 그래서 이게 왜 중요한데요?? => 이러한 경향이 문장에서도 나타남
- 나는 맛있는 빨간 사과를 먹었다.
	- 사과의 의미가 '맛있는'과 '빨간'에 의해서 의미가 변함
- 나는 맛없는 초록 사과를 먹었다.
	- 사과의 의미가 '맛없는'과 '초록'에 의해서 의미가 변함
- 자 나는 언어모델이고 다음 문장의 다음 단어를 예측해보자
	- 나는 맛있는 빨간 사과를 ???
		- 여기서는 '먹었다'라는 단어가 올 확률이 높을 거임
	- 너는 맛없는 초록 사과를 ???
		- 여기서는 '버렸다'라는 단어가 올 확률이 높음
	- 자 이 예시에서 같은 단어 '사과를'라는 단어의 다음 단어를 예측하지만 문장내 다른 단어에 의해 사과의 의미가 달라짐
		- 맛있는 -> 긍정적
		- 맛없는 -> 부정적
	- 하지만 '나는'과 '너는'이라는 단어는 '사과'라는 단어를 해석하는 큰 의미가 없음
		- 지금은 없지만 문장에 따라 달라질 수 있음
	- 이러한 정보를 컨택스트 정보라고 함
		- 이러한 관계를 찾와 영향력을 찾는게 Self Attention임
		- 어떤 단어가 어떤 단어를 해석하는 영향을 주는지 찾아야함
		- 단어의 의미 변경 = 벡터의 이동
### 정리
$W_q$ => 단어와 관계있는 단어를 찾는 질문임 
- 관심받고 싶은 사람에게 손을 들도록 만드는 것
    - 동사로 보는게 좀 더 적합한 예인것 같다
- 예시 
	- 명사는 형용사에 큰 영향을 받는다
	- 동사는 부사에 영향을 받는다
$W_k$ => 위 질문에 대한 답변의 정도
- 손을 든 사람에게 관심을 주는 의지를 수치화? 한 것
    - 이해관계나 여러 조건에 따라 관심을 줄 사람과 주기 싫은 사람이 있음 그리고 사람마다 주는 관심의 양도 조금씩 다른 거임
- 예시
	- 같은 형용사이지만 '빨간'이란 '초록'은 큰 영향이 없어 조금만 이동해야함
	- '맛있는'과 '맛없는'은 큰 영향을 미치기 떄문에 많이 이동해야함
$W_v$ => 얼만큼 이동해야 하지
- 실제로 관심을 표현하는 방법? 힘? 영향력?
- 예시
	- '맛없는'은 부정적인 방향으로 이동해야함
	- '빨간'은 빨간색 의미만큼 이동해야함
=> '빨간'은 빨간색 의미만큼 조금만 이동
'맛없는'은 맛없는 의미 만큼 큰게 이동
> 같은 것을 좋아하는 사람(단어)이라도 관심의 정도($QK^T$)가 다르고 주는 방식(V)도 다를다. 최종적으로 주는 관심 = 어텐션 점수

### Self Attention의 역할
- 위 과정을 n X n으로 빠르게 계산하는 $QK^T$
- 하나의 단어서 전체 단어의 영향의 합을 1로 만드는게 소프트 맥스 취한 값 $softmax({{QK^T}\over {\sqrt {d_k}}})$
	- $QK^T$의 값이 너무 커져서 softmax를 통과하면 기울기가 너무 작아지는 것을 방지 => 스케일링용
- 어디로 얼만큼 이동하는지 계산하는게 $softmax({{QK^T}\over {\sqrt {d_k}}})V = \overrightarrow {E}$
- 위 결과 값 $\overrightarrow {E}$ 가 <span style="color:rgb(255, 0, 0)">하나의 단어</span>가 <span style="color:rgb(0, 176, 240)">문장 전체</span>에 대해서 의미적으로 주는 <span style="color:rgb(146, 208, 80)">힘(영향)</span>임
	- 즉 모든 단어에 대해서 $\overrightarrow {E}$의 합이 문장의 컨택스트 정보가 될 수 있음
- 
> 위 과정의 Self Attention을 하는 이유이다

## 인코더의 self attention
- 위 과정이랑 같음
- 위 과정을 통해서 입력 문장에서 단어간의 영향력을 찾고 컨텍스트 정보를 찾음
## 디코더에서 Attention
### 1. Masked Attention
- 디코더의 입력으로 들어오는 문장에서 단어는 자신의 이전 단어들만 보고 예측하여야 하기때문에 이후 단어와의 관계가 계산에 포함되면 안됨
- 예시

|Query\Key|`<SOS>`|
|---|---|
|`<SOS>`|✅|

| Query\Key | `<SOS>` | `y₁` |
| --------- | ------- | ---- |
| `<SOS>`   | ✅       | ❌    |
| `y₁`      | ✅       | ✅    |

|Query\Key|`<SOS>`|`y₁`|`y₂`|
|---|---|---|---|
|`<SOS>`|✅|❌|❌|
|`y₁`|✅|✅|❌|
|`y₂`|✅|✅|✅|

- y1을 예측할 때 y2를 모르고 예측하였기 때문에 계산에서 제외되어야함 => 영향을 받지 않아야함 => Self Attention이 문장에 대해서 모든 단어간의 관계를 계산하는 과정에서 발생하는 오류
	- 계산에서 제외 시키는 방법은 ${QK^T} \over {\sqrt {d_k}}$의 값을 강제로 음의 무한대로 변경하면 소프트맥스에서 0이 되도록 한다.
### 2. Encoder-Decoder Attention
- 디코더의 어텐션 K와 Y값을 인코더의 출력에서 받아온다. 이 출력은 모든 디코더로 전달 된다. 
	- 이전 레이어(Mask Attention)의 출력에 Wq를 곱하여 Q로 사용하다
- => 현재 번역중인 문장의 단어 전체가 입력 문장의 단어 전체와 어떤 관계를 가지는가 => $softmax (QK^T) = T_y \times T_x$
- => 얼마만큼의 의미를 이동해야 하는가 => $softmax ({{QK^T} \over {\sqrt {d_k}}})V = T_y \times d_k$
## Multi-Head Attention
- $d_{model}$을 N개로 나누어 각각 다른 가중치($W_q, W_k, W_v$)를 가진 어텐션 레이어로 연산한뒤 concat(연결)하여 리니어 층을 통과하여 출력을 반환하는 방법
	- $T_x \times d_{model} => n * T_x \times d_{model \over n}$
- 헤드의 개수는 $d_k$를 결정하는 중요한 하이퍼 파라미터이다
	- $d_k = d_{modle} \div num\_heads$
#### 사용하는 이유
- 한번에 어텐션을 진행하여 softmax함수 때문에 적은 수의 강한 관계만 나타남
	- but, 특성을 나누어서 진행하면 더 많은 관계를 찾을 수 있다.
	- $W_o$를 통과하여 특성이 잘 부각될 수 있도록 한다.
## Positional Encoding
- 언어에서 RNN을 쓰는 이유? 
	- 현재의 결과가 다음 결과에 영향을 줌
	- 하지만, dot-product Attention은 한번에 관계를 연산함으로 그러한 정보가 없음
	- 그래서 강제로 추가시킨다 => Positional Encoding
###### 강제로 값을 추가하면 단어의 의미를 가진 임베딩 벡터의 의미가 바뀌지 않나요??
-> 맞음 그래서 Positional Encoding은 3가지 조건을 만족하는 값을 선택함
조건
1. 토큰에 많은 변화를 주지않은 작은 값이여야함
2. 모든 위치에서 서로 다른 값을 가져야함
3. 가까운 위치에 있는 값은 유사한 값을 가져야함

$$
\begin{align*}
PE_{(pos, 2i)} &= \sin\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right) \\
PE_{(pos, 2i+1)} &= \cos\left(\frac{pos}{10000^{2i/d_{\text{model}}}}\right)
\end{align*}
$$

pos는 단어의 위치이고 i는 임베딩의 차원입니다

-  토큰에 많은 변화를 주지않은 작은 값이여야함
	- 사인코사인은 -1과 1 사이의 값을 가진다
- 모든 위치에서 서로 다른 값을 가져야함
	- 사인 코사인은 주기함수인데요????
	- 그래서 단어의 위치와 차원에 따라 다른 주기를 사용!
- 가까운 위치에 있는 값은 유사한 값을 가져야함
	- 사인코사인은 연속적이기 때문에 가능

![image.png](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*8Max8j1xte0GXxkrnqNOow.png)