# 등장배경
- 기존의 언어 모델을 지도학습에 강하게 의존했음
	- 즉, 레이블된 문장만 학습할 수 있다.
	- 하지만 현실에서는 레이블된 데이터 보다 레이블 되진 않은 문장이 더 많음
- 비지도 학습을 사용하는 모델을 대한 연구는 계속 되고 있었지만 여기에는 두 가지 난제가 있음
	1. 전이 학습에 적합한 문장 표현을 효과적으로 학습하는 방법의 부재 => 알맞은 문장을 학습하는 방법
	2. 실제 과제에서 효율적으로 전이학습할 방법의 부재 => 실전에 사용하는 방법
		- 여기서 실전 문제란 문장 분류, 유사도, 텍스트 추론, 질문 답변등 있음
### 그 당시 언어 모델의 현실
- GPT-1이 나올 당시에 언어모델의 상황에 대해서 잠깐 설명하고 GPT가 무엇을 바꾸었는지 알아보자
#### 준지도 학습 (Semi-supervised Learnig)
- 준지도 학습이란
	- 소량의 지도학습과 다량의 비지도 학습을 이용하여 모델을 훈련하는 방식
	- 적은 양의 레이블된 데이터와 비레이블 데이터를 이용하여 성능을 향상하자 => 비레이블된 데이터를 활용하려는 노력
	- 모델의 시작점을 지도학습으로 잡고 비지도 학습을 이용하여 성능을 개선하자
- 하지만 모델의 성능(이해 수준)이 단어나 구문 수준 밖이 미치지 못함
	- 더 큰 문장 수준의 의미를 이해하지 못하는 문제가 있었다
#### 비지도 학습
- 레이블 없는 데이터를 활용하여 모델의 성능을 올리자
	- 여기서 최대 난제는 좋은 초기 지점(방향)을 잡도록 하는 것임
	- 예시를 들리자면 사진 100개를 주고 분류하고 한다.
		- 이때 비지도 학습의 문제는 몇개의 클래스로 분류하냐? 10개?? 5개??? 
		- 고양이를 구분하는데 패르시안 고양이와 샴 고양이도 다른 클래스로 구분해야 되나 같은 고양이로 분류해야하나?? 와 같은 문제
- 이때 가장 최근에 연구된 방식인 텍스트 분류 모델을 위해서 비지도 학습을 한 후 파인 튜닝으로 목적을 명확히 하는 학습 방법을 사용중 이였다.
---
- 위 상황에서 각 학습 방법의 문제점이 **"모델이 단어나 구문 수준의 의미 밖이 이해하지 못한다"**이다.
	- why? 전부 LSTM을 사용하여 먼 거리에 있는 의미를 이해하지 못하기 때문에 단어나 구문수준의 이해밖이 영향을 미치지 못함
	- ???: 이거 Transform쓰면 다 해결되는 거 아님???
- open ai는 Transform 구조를 이용하여 모델이 단어나 구문이 아닌 더 광범위한 문장에서 의미를 이해하여 자연어 추론, 의역 감지, 스토리 완성 등 더 광범위한 작업에서 좋은 성능을 낼 수 있도록 함
# 아이디어
- 학습을 두 단계로 나누어서 진행함
	- 사전 학습
	- 미세 조정
- 두 학습의 역할이 다르기 때문
##  사전 학습
- 사전 학습의 목표는 단어간의 관계를 학습하는 단계이다.
- 이전 단어들을 보고 다음 단어를 예측할 수 있도록 학습

- 레이블 되지 않은 전체 문장에서 일부 단어를 가리고 가려진 단어를 맞추도록 디코더를 학습하는 방식
## 미세조정
- 이때는 지도학습을 사용한다.

![image.png](https://velog.velcdn.com/images%2Fcau_dslab%2Fpost%2F221f13f2-d77c-400d-b6de-279a0479be44%2Fimage.png)

- 위와 같은 방식으로 입력 문장을 주고 모델의 결과을 보고 loss에 따라서 역전파를 이용하여 모델이 task를 잘 수행할 수 있도록 미세조정한다.
- 이 미세조정 hit인게 모델은 큰 구조 변경 없이 필요한 작업에 따라 미세조정이 가능한 점이다. 
	- 분류, 생성, 유사도 전부 다른 작업이지만 GPT는 입력하고 출력만 조정하면다 다 할 수 있다.
	- 이 말은 다시 말해서 다른 작업에 같은 사전 학습된 모델을 사용할 수 있다는 말임
# 아키텍쳐

![image.png](https://upload.wikimedia.org/wikipedia/commons/thumb/5/51/Full_GPT_architecture.svg/500px-Full_GPT_architecture.svg.png)

### Only Decoder 모델
- GPT의 학습 목표는 이전 단어를 보고 다음 단어를 정확히 예측하는 것임. 
	- 이러한 역할을 디코더의 역할임
** 그리고**
- 번역이 아니기 때문에 다른 문장의 문맥 정보를 가져올 필요가 없다
		- 따라서 인코더가 제거됨
		- 디코더 내부에 있었던 Encoder-Decoder Attention이 제거됨
> 위와 같은 이유로 디코더만 사용하여 모델을 구성함

## 장점
##### 1. 레이블 되지 않은 데이터를 이용하여 모델을 학습하고 성능을 개선할 수 있다
##### 2. 미세조정 단계에서 모델의 큰 변화 없이 다양한 task를 수행할 수 있도록 학습 할 수 있다

## 단점
##### 1. 사전학습에서 매우 많은 양의 학습이 필요하다.
- 사실 비지도 학습으로 단어간의 관계를 찾아야 하기 때문에 좋은 성능을 위해서 매우 많은 양의 데이터가 필요함
	- 현재 LLM보다는 매우 작은 데이터이지만 당시에 기술적 상황을 고려하면 매우 많은 양임
##### 2. 원하는 Task마다 별도의 지도 학습이 필요함
- 이미 사전학습에서도 많은 데이터를 학습했는데 또 task별로 학습이 필요함
	- 이 문제를 해결하여 만든 것이 GPT-2이다